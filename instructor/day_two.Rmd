---
title: 'Day Two: Data Munging'
author: "Dillon Niederhut"
date: "August, 19, 2015"
output: pdf_document
---

```{r, echo=FALSE}
knitr::opts_knit$set(root.dir = '../')
```

## Let's review what we learned yesterday
```{r}
# using a built-in data 
data(state)

# class of factors
state.division         # <Promise>
length(state.division)                       # how many elements in the object? # should be 50, obviously.
levels(state.division)                       # how many possible values?


# data Frames
# A data frame is a list of variables 
# of the same length with unique column names.
# When you work with data, you want to group your data in a data frame. 
state <- state.x77                    # rename the state.x77 object as state
rm(state.x77)                    # and remove the old object
state                            # shows you the whole dataset. often unnecessary


rownames(state)                 # what are the names of columns and rows?
names(state)                    # NULL! why? check the class of the object
                                # it is a matrix class object

state <- as.data.frame(state)                    # transform the matrix into a data frame
state               # looks quite much the same as before, but it's a data frame class now

```

## Introduction

Today's class will be essentially be split into two components: CRUD operations in R and TIDY data. For more on tidiness in data, see [Hadley Wickham's paper](www.jstatsoft.org/v59/i10/paper). We will also touch on missingness - for an accessible introduction, you can read [this very old and no longer state-of-the-art paper](http://psycnet.apa.org/journals/met/7/2/147/).

yesterday we saw how to create dataframes in R

```{r}
my.data <- data.frame(n = c(1, 2, 3),
                      c=c('one', 'two', 'three'),
                      b=c(TRUE, TRUE, FALSE),
                      d=c(as.Date("2015-07-27"), 
                          as.Date("2015-07-27")+7, 
                          as.Date("2015-07-27")-7),
                      really.long.and.complicated.variable.name=999)
```

remember, you can learn about dataframes with

```{r}
str(my.data)
```

in practice, you will only rarely create dataframes by hand, because creating tables in a text editor (or heaven forbid a command line) sucks

# Reading dataframes from file

## why read data from text files? 

they are human-readable and highly interaoperable

```{r}
read.table("data/mydata.csv", sep=',', header = TRUE)
```

## R has convenience wrappers for reading in tables

```{r}
read.csv("data/mydata.csv")
```

note that we are only reading the files by doing this

## R also has its own kind of data file

```{r}
load("data/mydata.Rda")
```

the `load` function does actually put the file into memory, and with the name you originally gave it when you saved it

this is typically a bad thing, and there is currently no easy workaround

## to read in tables from excel, use the `xlsx` package

if you are exporting data from excel, be sure to export datetimes as strings, as excel does not store dates internally the same way Unix does

```{r, eval=FALSE}
install.packages("xlsx")
library(xlsx)
read.xlsx("data/cpds_excel_new.xlsx")
```
But it may be better to save your .xlsx file as a csv. format in Excel first, and then read the csv file into R.

## you can also use R to read in data from proprietary software

```{r, eval=FALSE}
# examples of these?
install.packages("foreign")
library(foreign)
read.dta("data/cpds_stata.dta")
read.spss()
read.octave()
```

# data does not need to be in the local filesystem

# read text lines from a connection:

```{r, eval=FALSE}
RJ <- readLines("http://shakespeare.mit.edu/romeo_juliet/full.html")  
RJ[1:50]    # You now have the entire play of shakespeare's Romeo and Juliet
            # but you will need some data cleaning to have the text only
      
RJ[grep("<title>", RJ, perl=TRUE)]           # show me the line that contains the title of the play
RJ[grep("<h3>", RJ, perl=T)]                 # show me the first line of each act
RJ[grep("<h3>", RJ, perl=TRUE)]              # show me the first line of each scene
```



## R has an interface to curl called RCurl

```{r}
#install.packages('RCurl')
library(RCurl)
```

## you can use this to access remote data

```{r}
#install.packages("XML")
library(XML)

link <- "http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml"
page <- getURL(url = link)
xmlParse(file = page)
```

## R also has libraries for pulling and parsing web pages

```{r}
link<-"http://clerk.house.gov/evs/2014/ROLL_000.asp"
readHTMLTable(doc=link, header=T, which=1, stringsAsFactors=F)
```

# Connecting to a database

why read from a database? they use less memory, are faster, create their own backups, and offer optimized querying/joining

databases generally come in two flavors, relational and non-relational, which has to do with how important schemas are (and is a bit beyond the scope of an R intro)

two popular relational databases are SQL (or one of its many flavors)

```{r, eval=FALSE}
#are there websites that allow you to connect to test servers?
install.packages("RMySQL")
library(RMySQL)
con <- dbConnect(MySQL(),
         user="", password="",
         dbname="", host="localhost")
data <- fetch(dbSendQuery(con, "select * from table"), n=10)
con.exit(dbDisconnect(con))
```

and postgres

```{r, eval=FALSE}
install.packages("RPostgreSQL")
library(RPostgreSQL)
con <- dbConnect(dbDriver("PostgreSQL"),
                 dbname="", 
                 host="localhost",
                 port=1234, 
                 user="", 
                 password="")
data <- dbReadTable(con, c("column1","column2"))
dbDisconnect(con)
```

a popular non-relational database is MongoDB

```{r, eval=FALSE}
install.packages("rmongodb")
library(rmongodb)
con <- mongo.create(host = localhost, 
                      name = "", 
                      username = "", 
                      password = "", 
                      db = "admin")
if(mongo.is.connected(con) == TRUE) {
  data <- mongo.find.all(con, "collection", list("city" = list( "$exists" = "true")))
}
mongo.destroy(con)
```

one quirk about mongo is that your connection always authenticates to the authentication database, not the database you are querying - this db is usually called 'admin'

# Cleaning data

there are two major steps to data cleaning, which we will call 'sanitizing' and 'tidying'

in sanitizing, our goal is to take each variable and force its values to be honest representations of its levels

in tidying, we are arranging our data structurally such that each row contains exactly one observation, and each column contains exactly one kind of data about that observation (this is sometimes expressed in SQL terms as "An attribute must tell something about the key, the whole key, and nothing but the key, so help me Codd")

## exporting data from other software can do weird things to numbers and factors

```{r}
dirty <- read.csv('data/dirty.csv')
str(dirty)
```

## it's usually better to DISABLE R's intuition about data types

unless you already know the data is clean and has no non-factor strings in it (i.e. you are the one who created it)

```{r}
dirty <- read.csv('data/dirty.csv',stringsAsFactors = FALSE)
str(dirty)
```

## let's start by removing the empty rows and columns. I also drop the birth.order column.

```{r}
tail(dirty)
dirty <- dirty[1:5,-6]  
dirty <- dirty[ , -dim(dirty)[2]]     # what did I just do?
                                      # this is equivalent to dirty <- dirty[, c(1:4)]. Why?
dim(dirty)
```

## you can replace variable names

and you should, if they are uninformative or long

```{r}
names(dirty)
names(dirty) <- c("time", "height", "dept", "enroll")
```

## it's common for hand-coded data to have a signifier for subject-missingness 

(to help differentiate it from your hand-coder forgetting to do something)

```{r}
dirty$enrollment
```

## you should replace all of these values in your dataframe with R's missingness signifier, `NA`

```{r}
table(dirty$enroll)   # 999
                      # replace it with R's missingness signifier, NA
dirty$enroll[dirty$enroll=="999"] <- NA
table(dirty$enroll, useNA = "ifany")
```

> side note - read.table() has an option to specify field values as `NA` as soon as you import the data, but this is a BAAAAD idea because R automatically encodes blank fields as missing too, and thus you lose the ability to distinguish between user-missing and experimenter-missing


## that timestamp variable is not in a format R likes

base R doesn't handle time well, so we need to get rid of the time part of the timestamp

```{r}
dirty$time
dirty$time <- sub(' [0-9]+:[0-9]+:[0-9]+','',dirty$time)
dirty$time    # looks good!
```

## the height variable is in four different units. not good.

> side note - this is a complicated function - changing these line-by-line might be easier

we can fix this with a somewhat complicated loop (since R started as a functional language, there are not easy ways to conditionally modify structures in place)

```{r}
for (i in seq_along(dirty$height)) {
  if (grepl("’", dirty$height[i])) { # if in feet and inches
    nums <- lapply(strsplit(sub('"', "", dirty$height[[i]]), "’"), as.numeric)[[1]]
    dirty[i, 'height'] <- (nums[[1]] * 12 + nums[[2]]) * 2.5
  }
  if (grepl('"', dirty$height[i])) { # if in inches
    dirty[i, 'height'] <- as.numeric(sub('"', "", dirty$height[i])) * 2.5
  }
  if (is.na(as.numeric(dirty$height[i]))) { # if a character string
    dirty[i, 'height'] <- NA
  }
  else {
    nums <- as.numeric(dirty$height[i])
    if (nums <= 100 & nums > 3) { # if numeric inches
      dirty[i, 'height'] <- nums * 2.5 / 100
    }
    if (nums > 100) { # if numeric centimeters
      dirty[i, 'height'] <- nums/100
    }
  }
}
```

> side note: we can do the same task line-by-line, since the number of observations are small.

```{r}
class(dirty$height)
as.numeric(dirty$height)

# R reads comma as texts. We need to remove these.
dirty$height[grep("’", dirty$height, perl=TRUE)] <- 5*30.48 + 9*2.54
as.numeric(dirty$height)

# if no numeric value, then NA
dirty$height <- as.numeric(dirty$height)  # warning message! Why?

# recoding the element stored in inch
dirty$height[2] <- 70*2.54                # anyone knows what the last unit is?
dirty$height[3] <- 2.1*100                # meter -> centimeter
```


## let's fix some of those department spellings

first, let's make this all lowercase

```{r}
dirty$dept
dirty$dept <- tolower(dirty$dept)
dirty$dept <- gsub(' ', '', dirty$dept)  # what did we just do?
dirty$dept[4] <- "geology"
dirty[dirty == "999"] <- NA
```

## then, you can coerce the data into the types they should be

```{r}
dirty$time <- as.Date(dirty$time,'%m/%d/%Y')
dirty$height <- as.numeric(dirty$height)
dirty$dept <- as.factor(dirty$dept)
dirty$enroll <- as.factor(dirty$enroll)
str(dirty)
```

# Subsetting and merging

## an aside on testing

in R, you use double symbols for testing

```{r}
1 == 2
1 != 1
1 >= 1
```

(you've already seen a couple of these)

## tests return boolean vectors

```{r}
1 >= c(0,1,2)
```

## recall that boolean vectors need to be the same length or a divisor

if your vectors are not multiples of each other, R will fuss at you

```{r}
c(1,2) >= c(1,2,3)
c(1,2) >= c(1,2,3,4)     # why no warning this time? R recycles!
```

the combination of the length requirement, the lack of support in R for proper indexing, and missingness in your data will cause many headaches later on

## subsetting data frames

subsetting your data is where you will use this regularly

```{r}
my.data$numeric == 2
my.data[my.data$numeric == 2,]
```

## boolean variables can act as filters right out of the box

```{r}
my.data[my.data$b,]
```

you see the empty space after the comma? that tells R to grab all the columns

## you can also select columns

```{r}
my.data[,'d']
```

that empy space **before** the comma? that tells R to grab all the rows

## you can also match elements from a vector

```{r}
good.things <- c("three", "four", "five")
my.data[my.data$character %in% good.things, ]
```

## most subsetting operations on dataframes also return a dataframe

```{r}
str(my.data[!(my.data$character %in% good.things), ])
```

## subsets that are a single column return a vector

```{r}
str(my.data$numeric)
```

# Merging data frames

## flexibly join dataframes with `merge`

imagine you have two datasets that you want to merge

```{r}
data.1 <- read.csv('data/merge_practice_1.csv')
data.2 <- read.csv('data/merge_practice_2.csv')
str(data.1)
str(data.2)
```

sometimes the same people have differet jobs in different locations

you can do an *inner* join using merge

```{r}
merge(data.1, data.2, by = 'id')
```

that's no good - we lost half of our people! 

inner joins are mostly used when you **only** want records that appear in both tables

if you want the union, you can use an outer join

```{r}
merge(data.1, data.2, by = 'id', all = TRUE)
```

this works basically the same as `join` in SQL

running merges is particularly useful when:

a. your data is tidy; and,
b. you want to add information with a lookup table

in this case, you can store your lookup table as a dataframe, then merge it

```{r}
lookup <- read.csv('data/merge_practice_3.csv')
str(lookup)
```

this lookup table gives us the population for each location

we can add this to our people table with

```{r}
merge(data.1, lookup, by = "location")
```

note that Reno was in our lookup table

```{r}
lookup[lookup$location == 'Reno', ]
```

but doesn't show up when we merge - why do you think this is?

# Missingness

there are many reasons why you might have missing data

*AS LONG AS MISSINGNESS IS NOT CAUSED BY YOUR INDEPENDENT VARIABLE* this is fine

deleting those observations is wasteful, but easy (listwise deletion)

ignoring the individual missing data points is not bad (casewise deletion)

imputing mean values for missing data is possibly the worst thing you can do

imputing via MI + error is currently the best option

## listwise deletion is wasteful

```{r}
na.omit(dirty)
```

## casewise deletion is what R does internally

```{r}
nrow(dirty)
sum(is.na(dirty$height))
sum(is.na(dirty$birth.order))
length(lm(height ~ birth.order,data=dirty)$fitted.values)
```

this is usually the default strategy

## remember how we talked about the extensibility of R?

amelia is a package that makes a complicated MI approach stupidly easy

```{r}
library(Amelia)
```

## let's use this large dataset as an example

```{r}
large <- read.csv('data/large.csv')
summary(large)
nrow(na.omit(large))
```

## for it to work you need low missingness and large N

```{r}
a <- amelia(large,m = 1)
print(a)
```

## amelia returns a list, where the first item is a list of your imputations

we only did one, so here it is

```{r}
large.imputed <- a[[1]][[1]]
summary(large.imputed)
```

## if you give it a tiny dataset, it will fuss at you

```{r}
a <- amelia(large[990:1000,],m = 1)
print(a)
```

# Reshaping

now that our data is clean, it's time to put it in a tidy format. this is a way of storing data that makes it easy to:

1. make graphs
2. run tests
3. summarize
4. transform into other formats

we are basically trying to organize ourselves such that:

1. any grouping is made on rows
2. any testing is done between columns

## most tidying can be done with two R packages

```{r, eval=FALSE}
install.packages('reshape2')
install.packages('stringr')
install.packages('plyr')
```

```{r}
library(reshape2)
library(stringr)
library(plyr)
```

## let's grab some data from Pew

```{r}
library(foreign)
pew <- as.data.frame(read.spss("data/pew.sav"))
religion <- pew[c("q16", "reltrad", "income")]
rm(pew)
```

## we'll start by cleaning up the factor variables

```{r}
religion$reltrad <- as.character(religion$reltrad)
religion$reltrad <- str_replace(religion$reltrad, " Churches", "")
religion$reltrad <- str_replace(religion$reltrad, " Protestant", " Prot")
religion$reltrad[religion$q16 == " Atheist (do not believe in God) "] <- "Atheist"
religion$reltrad[religion$q16 == " Agnostic (not sure if there is a God) "] <- "Agnostic"
religion$reltrad <- str_trim(religion$reltrad)
religion$reltrad <- str_replace_all(religion$reltrad, " \\(.*?\\)", "")

religion$income <- c("Less than $10,000" = "<$10k", 
  "10 to under $20,000" = "$10-20k", 
  "20 to under $30,000" = "$20-30k", 
  "30 to under $40,000" = "$30-40k", 
  "40 to under $50,000" = "$40-50k", 
  "50 to under $75,000" = "$50-75k",
  "75 to under $100,000" = "$75-100k", 
  "100 to under $150,000" = "$100-150k", 
  "$150,000 or more" = ">150k", 
  "Don't know/Refused (VOL)" = "Don't know/refused")[religion$income]

religion$income <- factor(religion$income, levels = c("<$10k", "$10-20k", "$20-30k", "$30-40k", "$40-50k", "$50-75k", 
  "$75-100k", "$100-150k", ">150k", "Don't know/refused"))
```

## now we can reduce this down to three columns for three variables

```{r}
religion <- count(religion, c("reltrad", "income"))
names(religion)[1] <- "religion"
```

## notice how easy it is to subset now

```{r}
religion[religion$religion == 'Unaffiliated',]
```

but raw counts aren't very helpful for religons with widely different population sizes

## reshaping

```{r}
abnormal <- data.frame(name = c('Alice','Bob','Eve'),
                       time1 = c(90,90,150),
                       time2 = c(100,95,100))
normal <- melt(data = abnormal, id.vars = 'name')
normal$id <- seq(1:nrow(normal))
names(normal) <- c('name','time','value','id')
normal$time <- str_replace(normal$time,'time','')
normal[normal$time == 1,]
normal[normal$name == 'Alice',]
```

# the below was all stolen from Chris, and needs reworking

## enter plyr

- *plyr* is the go-to package for all your splitting-applying-combining needs
- Among its many benefits (above base R capabilities):
a) Don't have to worry about different name, argument, or output consistencies
b) Easily parallelized 
c) Input from, and output to, data frames, matricies, and lists
d) Progress bars for lengthy computation
e) Informative error messages

## group-wise operations/plyr/selecting functions

- Two essential questions:
    1. What is the class of your input object?
    2. What is the class of your desired output object?
- If you want to split a **d**ata frame, and return results as a **d**ata frame, you use **dd**ply
- If you want to split a **d**ata frame, and return results as a **l**ist, you use **dl**ply
- If you want to split a **l**ist, and return results as a **d**ata frame, you use **ld**ply


```{r, eval=FALSE}
# Using the appropriate plyr function (ddply), compute vote percentages for Kerry (pres04==1), Bush (pres04==2), Nader (pres04==3), and others (pres04==9)
ddply(.data=red.blue, .variables=.(race), .fun=summarize, 
      kerry=length(pres04[pres04==1])/length(pres04)*100, 
      bush=length(pres04[pres04==2])/length(pres04)*100, 
      nader=length(pres04[pres04==3])/length(pres04)*100, 
      other=length(pres04[pres04==9])/length(pres04)*100 
)
```

Consider the case where we want to calculate vote choice statistics across race from a data.frame, and return them as a list:

```{r, eval=FALSE}
dlply(.data=red.blue, .variables=.(race), .fun=summarize, 
      kerry=length(pres04[pres04==1])/length(pres04)*100, 
      bush=length(pres04[pres04==2])/length(pres04)*100, 
      nader=length(pres04[pres04==3])/length(pres04)*100, 
      other=length(pres04[pres04==9])/length(pres04)*100 
)
```

# Group-wise Operations/plyr/functions

- plyr can accomodate any user-defined function, but it also comes with some pre-defined functions that assist with the most common split-apply-combine tasks
- We've already seen **summarize**, which creates user-specified vectors and combines them into a data.frame.  Here are some other helpful functions:

**transform**: applies a function to a data.frame and adds new vectors (columns) to it

```{r, eval=FALSE}
# Add a column containing the average age of the race of the individual
red.blue.transform<-ddply(.data=red.blue, .variables=.(race), .fun=transform,
      race.avg.age=mean(x=age9, na.rm=T))
unique(red.blue.transform$race.avg.age)
```

Note that **transform** can't do transformations that involve the results of *other* transformations from the same call

```{r, eval=FALSE}
# Attempt to add new columns that draw on other (but still new) columns
red.blue.transform<-ddply(.data=red.blue, .variables=.(race), .fun=transform,
      race.avg.age=mean(x=age9, na.rm=T),
      race.avg.age.plusone=race.avg.age+1)
```

For this, we need **mutate**: just like transform, but it executes the commands iteratively so  transformations can be carried out that rely on previous transformations from the same call

```{r, eval=FALSE}
# Attempt to add new columns that draw on other (but still new) columns
red.blue.mutate<-ddply(.data=red.blue, .variables=.(race), .fun=mutate,
      race.avg.age=mean(x=age9, na.rm=T),
      race.avg.age.plusone=race.avg.age+1)
unique(red.blue.mutate$race.avg.age)
unique(red.blue.mutate$race.avg.age.plusone)
```

Another very useful function is **arrange**, which orders a data frame on the basis of column contents

```{r, eval=FALSE}
# Arrange by "age9", ascending
red.blue.age<-plyr::arrange(df=red.blue, age9)
red.blue.age[1:25, 1:5]
# Arrange by "age9", descending
red.blue.age<-plyr::arrange(df=red.blue, desc(age9))
red.blue.age[1:25, 1:5]
# Arrange by "age9" then "sex"
red.blue.age.sex<-plyr::arrange(df=red.blue, age9, sex)
red.blue.age.sex[1:25, 1:5]
# Arrange by "sex" (descending) then "age9"
red.blue.sex.age<-plyr::arrange(df=red.blue, desc(sex), age9)
red.blue.sex.age[1:25, 1:5]
```

# Acknowledgements

## Materials taken from:

[Chris Krogslund](https://github.com/ckrogs/r_useful_dlab)
[Hadley Wickham](www.jstatsoft.org/v59/i10/paper)
