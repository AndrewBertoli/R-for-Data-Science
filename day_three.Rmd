---
title: "day_three"
author: "Dillon Niederhut"
date: "July 27, 2015"
output: pdf_document
---

## Introduction

okay, time for the business. 

analysis here is going to procede in two steps:

1. exploratory data analysis
2. statistical inference

our treatment of graphing owes a lot to the Grammar of Graphics, 

# Summarizing

## let's load in some data about D-Lab feedback 

```{r}
load('data/feedback.Rda')
str(dat)
```

## R provides two easy/simple summary functions in the base package

```{r}
summary(dat)
table(dat$department)
```

think back to yesterday - how would we make weekdays out of the date variable?

```{r}
dat$wday <- factor(weekdays(dat$timestamp, abbreviate = TRUE), 
                   levels = c('Mon','Tue','Wed','Thu','Fri','Sat','Sun')
                   )
summary(dat$wday)
```

## reshape provides a few more ways to aggregate things

```{r}
library(reshape2)
dcast(dat[dat$gender == 'Female/Woman' | dat$gender == 'Male/Man',], department ~ gender)
dcast(melt(dat, measure.vars = c('course.delivered')), wday ~ 'Delivered', fun.aggregate = mean)
```

# Plotting

## every time you use `base::plot`, Edward Tufte kills a kitten

- we'll be using ggplot, R's implementation of the **grammar of graphics**

- in this grammar, you use 'aesthetics' to define how data is mapped to objects the graph space

- each graph space has at least three layers:
    - theme/background/annotations
    - axes
    - objects

- most objects are geometric shapes

- some objects are statistics built on those shapes

- you can stack as many layers as you like

```{r}
install.packages('ggplot2')
library(ggplot2)
```

## use qplot for initial poking around

it has very strong intuitions about what you want to see, and is not particularly customizable

```{r}
qplot(instructor.communicated, data = dat)
qplot(wday, course.delivered, data = dat)
```

## for 1D cateforical, use bar

```{r}
ggplot(data=dat, aes(x=wday)) + geom_bar()
```

## for 1D continuous, use hist

this is really just convenience for `geom_bar(stat = 'bin')`, as opposed to bar plots, whose `stat` is `'count'`

```{r}
ggplot(data=dat, aes(x=course.delivered)) + 
  geom_histogram(binwidth=1)
```

you can add color to this plot

```{r}
ggplot(data=dat, aes(x=course.delivered)) + 
  geom_histogram(binwidth=1, fill = 'gold', colour= 'blue')
```

GO BEARS

## for many 1D variables, use a box plot

these are handy for a whole bunch of reasons, and you should make them your close associates

```{r}
ggplot(data=dat, aes(x=gender,y=interest)) + geom_boxplot()
```

## to plot two continuous variables, use points

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + geom_point()
```

all of these values are discrete, which makes them hard to see

## to scatter points randomy, use jitter

this is really just convenience for `geom_point(position = jitter())` 

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter()
```

not only can you add color, you can make the color a mapping of other variables

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter(aes(colour = wday))
```

the last time we used `colour` it was not an aesthetic - why is it now?

## you can stack layers until your eyes hurt

```{r}
ggplot(data=dat, aes(x=wday, y=course.delivered)) + 
  geom_boxplot(colour = 'gold') + 
  geom_jitter(colour = 'blue')
```

## add summary functions with smooth

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm')
```

if you are using colour as an aesthetic, you'll produce stats for each color

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered, colour = wday)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm', se = FALSE)
```

## good scientists put units on their axes

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm', colour = 'black') + 
  xlab('How well the instructor communicated (1-7)') + 
  ylab('How well the course delivered advertised content (1-7)') + 
  ggtitle("I have no idea what I'm doing") 
```

the general point here is that every single object on this graph is customizable

frequent customizations are very simple to add

infrequent customizations will take a lot of tinkering your part

## facetting

often useful for looking at relationships between three variables at the same time

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm') +
  facet_grid(. ~ useful)
```

# Mean testing

a picture is worth 1,000 words, but a p-value is worth a dissertation

basically, inferential statistics is the application of probability theory to decide what is real and what isn't

we'll start by trying to tell whether differences between group summaries are real

## t.test with two vectors (default method)

```{r}
t.test(dat$inside.barriers, dat$outside.barriers)
```

note that R takes care of the defaults for you - what it is really computing is `t.test(dat$inside.barriers, dat$outside.barriers, alternative = "two.sided", paired = FALSE, var.equal = FALSE, mu = 0, conf.level = 0.95)

how would you find this out for yourself?

## t.test with subsets of one vector (default method)

```{r}
t.test(dat$outside.barriers[dat$gender == "Male/Man"], dat$outside.barriers[dat$gender == "Female/Woman"])
```

remember all that 'different kinds of objects have different methods' crap?

## t.test with S3 method

```{r}
t.test(outside.barriers ~ gender, data = dat, subset = dat$gender %in% c("Male/Man", "Female/Woman"))
```

## aov

first, you would think anova would be called by `anova`, but that's reserved for conducting F-tests on lm objects

second, you really shouldn't be using anova in the first place, but if you must, the syntax looks like this

```{r}
aov(outside.barriers ~ gender, data = dat)
```

this isn't particularly helpful, but remember that it is an object, and we can call other, more helpful functions, on that object

remember our old friend `summary`? it works on almost everything

```{r}
model.1 <- aov(outside.barriers ~ gender, data = dat)
summary(model)
```

that's a little better - but what about post-hoc testing?

```{r}
TukeyHSD(model.1)
```

# linear models

mean tests are really just a subset of linear models where one of your variables is a category

## cor.test (Pearson)
 
earlier, we were looking at differences between the means of two variables

but those variables were both continuous, so we can ask whether they are related

```{r}
cor.test(dat$outside.barriers, dat$inside.barriers)
```

okay, so they're related - now what?

## lm

this is probably the closest you will get to building a linear model by hand

this means lm is a powerful tool, but you have to know what you're doing

the basic call is the S3 method

```{r}
model.1 <- lm(inside.barriers ~ outside.barriers, data = dat)
summary(model.1)
```

## R automatically one-hot encodes your categories

```{r}
model.2 <- lm(inside.barriers ~ outside.barriers + department, data = dat)
summary(model.2)
```

## R does not assume you want the full factorial model

```{r}
model.3 <- lm(inside.barriers ~ outside.barriers + department + outside.barriers*department, data = dat)
summary(model.3)
```

## extract model parameters with `$`

```{r}
model.1$coefficients
model.1$coefficients[[2]]
```

## this is useful if you want to plot residuals

```{r}
dat$residuals <- model.1$residuals
```

wait crap! remember how we talked about R having casewise deletion + crappy indexing? this is where it hurts

we have to do something like this:

```{r}
dat.listwise <- dat[!is.na(dat$inside.barriers) & !is.na(dat$outside.barriers), ]
dat.listwise$resid <- model.1$residuals
```

then we can do this

```{r}
ggplot(data = dat.listwise, aes(x=gender,y=resid)) + 
  geom_boxplot()
```

# Nonparametric

parametric refers to using means, deviations, and other estimates of population parameters

*BUT* what if you don't want to make assumptions about the structure of the population?

or what if you **gasp** can't?

## ranked variables

a simple case is where means don't have meaning

above we were looking at correlations between Likert variables

all Likerts are really rank variables, which means they don't act like actual number-y numbers

in the real world, an 6 foot tall person is twice as tall as a 3 foot tall person

but is a level '6' really twice as many barriers to access as a '3'?

**NOPE**

we know that 6 is more than 3, but can't really say how much - in that sense then, a scale of 1-7 is exactly the same thing as a scale of a-g.

## median testing ranks

we use Mann-Whitney sums to test that the ranks are centered the same way

```{r}
wilcox.test(dat$outside.barriers, dat$inside.barriers, alternative = "two.sided", paired = FALSE, mu = 0, conf.level = 0.95)
```

see how this setup looks exactly like a t-test? that's not an accident

## correlating ranks

this is just like the `cor.test` you did above, but with `method` set to equal 'spearman' instead of pearson

```{r}
cor.test(dat$outside.barriers, dat$inside.barriers, method = 'spearman')
```

rho is pretty close to the r from above

## chisq

what if both of your variables are categories? we can test their counts with R's built in `chisq.test` function

i.e. what if we want to know if gender is distributed evenly over departments?

```{r}
chisq.test(dat$gender, dat$department)
```

surprising no one, it is not

# Acknowledgements

## Materials taken from:

[D-Lab's Feedback Analytics](https://github.com/dlab-berkeley/feedback-analytics)